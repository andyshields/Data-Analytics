{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Regression Algorithms\nYou cannot know which algorithms are best suited to your problem beforehand. You must trial a number of methods and focus attention on those that prove themselves the most promising. In this lab you will discover six machine learning algorithms that you can use when spot-checking your regression problem in Python with scikit-learn. After completing this lesson you will know:\n1. How to spot-check machine learning algorithms on a regression problem.\n2. How to spot-check four linear regression algorithms.\n3. How to spot-check three nonlinear regression algorithms.\n<br>\n\nLet’s get started.\n<br>\n\nAlgorithms Overview\nIn this lesson we are going to take a look at seven regression algorithms that you can spot-check on your dataset. Starting with four linear machine learning algorithms:\n- Linear Regression.\n- Ridge Regression.\n- LASSO Linear Regression.\n- Elastic Net Regression.\n<br>\n\nThen looking at three nonlinear machine learning algorithms:\n- k-Nearest Neighbors.\n- Classiﬁcation and Regression Trees.\n- Support Vector Machines."
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Linear Machine Learning Algorithms\nEach recipe is demonstrated on the Boston House Price dataset. This is a regression problem where all attributes are numeric. A test harness with 10-fold cross-validation is used to demonstrate how to spot-check each machine learning algorithm and mean squared error measures are used to indicate algorithm performance. Note that mean squared error values are inverted (negative). This is a quirk of the cross val score() function used that requires all algorithm metrics to be sorted in ascending order (larger value is better). The recipes assume that you know about each machine learning algorithm and how to use them. We will not go into the API or parameterization of each algorithm.\n\nThis section provides examples of how to use four diﬀerent linear machine learning algorithms for regression in Python with scikit-learn.\nhttp://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html"
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## Linear Regression\nLinear regression assumes that the input variables have a Gaussian distribution. It is also assumed that input variables are relevant to the output variable and that they are not highly correlated with each other (a problem called collinearity). You can construct a linear regression model using the LinearRegression class.\n<br>\nRunning the example provides a estimate of mean squared error."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Linear Regression \nfrom pandas import read_csv \nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.linear_model import LinearRegression \nfilename = 'housing.csv' \nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'] \ndataframe = read_csv(filename, delim_whitespace=True, names=names) \narray = dataframe.values \nX = array[:,0:13] \nY = array[:,13] \nkfold = KFold(n_splits=10, random_state=7) \nmodel = LinearRegression() \nscoring = 'neg_mean_squared_error' \nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring) \nprint(results.mean())",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "-34.70525594452486\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## Ridge Regression\nRidge regression is an extension of linear regression where the loss function is modiﬁed to minimize the complexity of the model measured as the sum squared value of the coeﬃcient values (also called the L2-norm). You can construct a ridge regression model by using the Ridge class2.\n2http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html \n<br>\n\nRunning the example provides an estimate of the mean squared error."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Ridge Regression \nfrom pandas import read_csv \nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.linear_model import Ridge \nfilename = 'housing.csv' \nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'] \ndataframe = read_csv(filename, delim_whitespace=True, names=names) \narray = dataframe.values \nX = array[:,0:13] \nY = array[:,13] \nkfold = KFold(n_splits=10, random_state=7)\nmodel = Ridge() \nscoring = 'neg_mean_squared_error' \nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring) \nprint(results.mean())",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": "-34.07824620925938\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## LASSO Regression\nThe Least Absolute Shrinkage and Selection Operator (or LASSO for short) is a modiﬁcation of linear regression, like ridge regression, where the loss function is modiﬁed to minimize the complexity of the model measured as the sum absolute value of the coeﬃcient values (also called the L1-norm). You can construct a LASSO model by using the Lasso class3.\nhttp://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html \n<br>\n\nRunning the example provides an estimate of the mean squared error."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Lasso Regression \nfrom pandas import read_csv \nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.linear_model import Lasso \nfilename = 'housing.csv' \nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'] \ndataframe = read_csv(filename, delim_whitespace=True, names=names) \narray = dataframe.values \nX = array[:,0:13] \nY = array[:,13]\nkfold = KFold(n_splits=10, random_state=7) \nmodel = Lasso() \nscoring = 'neg_mean_squared_error' \nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring) \nprint(results.mean())",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": "-34.46408458830231\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## ElasticNet Regression\nElasticNet is a form of regularization regression that combines the properties of both Ridge Regression and LASSO regression. It seeks to minimize the complexity of the regression model (magnitude and number of regression coeﬃcients) by penalizing the model using both the L2-norm (sum squared coeﬃcient values) and the L1-norm (sum absolute coeﬃcient values). You can construct an ElasticNet model using the ElasticNet class4.\nhttp://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html\n<br>\nRunning the example provides an estimate of the mean squared error."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# ElasticNet Regression \nfrom pandas import read_csv \nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.linear_model import ElasticNet \nfilename = 'housing.csv' \nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'] \ndataframe = read_csv(filename, delim_whitespace=True, names=names) \narray = dataframe.values \nX = array[:,0:13] \nY = array[:,13] \nkfold = KFold(n_splits=10, random_state=7) \nmodel = ElasticNet() \nscoring = 'neg_mean_squared_error' \nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring) \nprint(results.mean())",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": "-31.16457371424975\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Nonlinear Machine Learning Algorithms\nThis section provides examples of how to use three diﬀerent nonlinear machine learning algorithms for regression in Python with scikit-learn."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "\n## K-Nearest Neighbors\nThe k-Nearest Neighbors algorithm (or KNN) locates the k most similar instances in the training dataset for a new data instance. From the k neighbors, the mean or median output variable is taken as the prediction. Of note is the distance metric used (the metric argument). The Minkowski distance is used by default, which is a generalization of both the Euclidean distance (used when all inputs have the same scale) and Manhattan distance (used when the scales of the input variables diﬀer). You can construct a KNN model for regression using the KNeighborsRegressor class.\nhttp://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\n<br>\nRunning the example provides an estimate of the mean squared error."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# KNN Regression \nfrom pandas import read_csv \nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.neighbors import KNeighborsRegressor \nfilename = 'housing.csv' \nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'] \ndataframe = read_csv(filename, delim_whitespace=True, names=names) \narray = dataframe.values \nX = array[:,0:13] \nY = array[:,13] \nkfold = KFold(n_splits=10, random_state=7) \nmodel = KNeighborsRegressor() \nscoring = 'neg_mean_squared_error' \nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring) \nprint(results.mean())",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": "-107.28683898039215\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Classiﬁcation and Regression Trees\nDecision trees or the Classiﬁcation and Regression Trees (CART as they are known) use the training data to select the best points to split the data in order to minimize a cost metric. The default cost metric for regression decision trees is the mean squared error, speciﬁed in the criterion parameter. You can create a CART model for regression using the DecisionTreeRegressor class6.\nhttp://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n<br>\nRunning the example provides an estimate of the mean squared error."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Decision Tree Regression \nfrom pandas import read_csv \nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.tree import DecisionTreeRegressor \nfilename = 'housing.csv'\nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'] \ndataframe = read_csv(filename, delim_whitespace=True, names=names) \narray = dataframe.values \nX = array[:,0:13] \nY = array[:,13] \nkfold = KFold(n_splits=10, random_state=7) \nmodel = DecisionTreeRegressor() \nscoring = 'neg_mean_squared_error' \nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring) \nprint(results.mean())",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": "-34.368815686274516\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Support Vector Machines\nSupport Vector Machines (SVM) were developed for binary classiﬁcation. The technique has been extended for the prediction real-valued problems called Support Vector Regression (SVR). Like the classiﬁcation example, SVR is built upon the LIBSVM library. You can create an SVM model for regression using the SVR class7.\nhttp://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n<br>\nRunning the example provides an estimate of the mean squared error."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# SVM Regression \nfrom pandas import read_csv \nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score \nfrom sklearn.svm import SVR \nfilename = 'housing.csv' \nnames = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV'] \ndataframe = read_csv(filename, delim_whitespace=True, names=names) \narray = dataframe.values \nX = array[:,0:13] \nY = array[:,13] \nkfold = KFold(n_splits=10, random_state=7) \nmodel = SVR() \nscoring = 'neg_mean_squared_error' \nresults = cross_val_score(model, X, Y, cv=kfold, scoring=scoring) \nprint(results.mean())\n\n\n",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": "-91.04782433324428\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Summary\nIn this lab you discovered how to spot-check machine learning algorithms for regression problems in Python using scikit-learn. Speciﬁcally, you learned about four linear machine learning algorithms: Linear Regression, Ridge Regression, LASSO Linear Regression and Elastic Net Regression. You also learned about three nonlinear algorithms: k-Nearest Neighbors, Classiﬁcation and Regression Trees and Support Vector Machines."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}